{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial News Headline Sentiment Classifier\n",
    "\n",
    
    "\n",
    "**Motivation**\n",
    "Market sentiment derived from news articles is one factor that can influence trading and investment decisions. By understanding the sentiment of financial news headlines, investors, traders, and financial institutions have another tool that they can use to make more informed decisions, assess risks, and potentially even design algorithmic trading strategies. This notebook demonstrates the process of training a BERT (Bidirectional Encoder Representations from Transformers) model on the financial_phrasebank dataset that will be used to classify Marketwatch.com headlines into three sentiment categories: positive, neutral, and negative. While this exercise uses financial headlines as an input, the underlying techniques used in this notebook can be transferred to simmilar text classification tasks.\n",
    "The methods outlined in this notebook also serve as a less-complicated, indirect sample for my actual data science work making a similar transformer model for multilabel text classification.\n",
    "\n",
    "**Data Source**\n",
    "The financial_phrasebank dataset is a collection of financial news headlines for companies listed in OMX Helsinki, each labeled with a sentiment score of 2 for positive, 1 for neutral, or 0 for negative. Labels were decided upon by a group of 13 annotators at the  Aalto University School of Business in which 75% of the judges agree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Aum Thaker\\Desktop\\VSC\\jupyter\\myvenv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyodbc as pb\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from transformers import AutoTokenizer\n",
    "import heapq\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import Dataset\n",
    "from transformers import (BertForSequenceClassification, \n",
    "                          AdamW, \n",
    "                          BertConfig,\n",
    "                          AutoModel, \n",
    "                          AutoModelForSequenceClassification,\n",
    "                          BitsAndBytesConfig, \n",
    "                          AutoModelForCausalLM,\n",
    "                          TrainingArguments, \n",
    "                          Trainer, \n",
    "                          EarlyStoppingCallback)\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import multiprocessing as mp\n",
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "from peft import (LoraConfig,\n",
    "                  LoraModel,\n",
    "                  get_peft_model,)\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "import bitsandbytes as bnb\n",
    "from bitsandbytes.optim import AdamW8bit\n",
    "from torch import nn\n",
    "from transformers.trainer_pt_utils import get_parameter_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all dataframe columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "# Load the datasets into a pandas dataframe\n",
    "headlines = pd.DataFrame( load_dataset(\"financial_phrasebank\", \"sentences_75agree\",trust_remote_code=True)[\"train\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>For the last quarter of 2010 , Componenta 's n...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In the third quarter of 2010 , net sales incre...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Operating profit rose to EUR 13.1 mn from EUR ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0  According to Gran , the company has no plans t...      1\n",
       "1  With the new production plant the company woul...      2\n",
       "2  For the last quarter of 2010 , Componenta 's n...      2\n",
       "3  In the third quarter of 2010 , net sales incre...      2\n",
       "4  Operating profit rose to EUR 13.1 mn from EUR ...      2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headlines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3453 entries, 0 to 3452\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   sentence  3453 non-null   object\n",
      " 1   label     3453 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 54.1+ KB\n"
     ]
    }
   ],
   "source": [
    "headlines.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsMAAAIjCAYAAADmyBbAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/TGe4hAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA0fUlEQVR4nO3deVxVdeL/8fdldwNCBcQI19y3XHE3SVS0nOxRmrmN6eSgk1matLi1OFlpWUzWd0aprEmz0r5aJoJLGW4YLqROOq4pYCJcsNzg/P7oy/11RU0IuMjn9Xw87uPRPedzz/0cztx4zencg82yLEsAAACAgdxcPQEAAADAVYhhAAAAGIsYBgAAgLGIYQAAABiLGAYAAICxiGEAAAAYixgGAACAsYhhAAAAGIsYBgAAgLGIYQC4AUeOHJHNZtMrr7xSYtvcsGGDbDabNmzYUGLbLDBz5kzZbLYS3+7V9OzZUz179nQ8L9iv5cuXl8n7jxo1SnXq1CmT9wJQ8RDDACqsuLg42Ww27dixw9VT+UMK9qPg4ePjo5CQEEVGRmrBggXKyckpkfc5efKkZs6cqZSUlBLZXkkqz3MDcHMjhgHgJjF79my9//77euuttzRx4kRJ0qRJk9SiRQvt3r3baewzzzyjX375pUjbP3nypGbNmlXk4Fy7dq3Wrl1bpNcU1fXm9j//8z86cOBAqb4/gIrLw9UTAADcmH79+qldu3aO5zExMUpMTNSAAQN09913a9++fapUqZIkycPDQx4epfuv+J9//lmVK1eWl5dXqb7P7/H09HTp+wO4uXFmGIDRLl68qOnTp6tt27by8/NTlSpV1K1bN61fv/6ar5k/f77CwsJUqVIl9ejRQ3v37i00Zv/+/brvvvsUEBAgHx8ftWvXTp9//nmJz//OO+/Us88+q6NHj2rJkiWO5Ve7Zjg+Pl5du3aVv7+/qlatqkaNGumpp56S9Ot1vu3bt5ckjR492nFJRlxcnKRfrwtu3ry5kpOT1b17d1WuXNnx2iuvGS6Ql5enp556SsHBwapSpYruvvtuHT9+3GlMnTp1NGrUqEKv/e02f29uV7tm+Ny5c3r88ccVGhoqb29vNWrUSK+88oosy3IaZ7PZNGHCBK1YsULNmzeXt7e3mjVrpjVr1lz9Bw6gwuHMMACj2e12/fOf/9TQoUM1duxY5eTk6F//+pciIyO1bds2tW7d2mn8e++9p5ycHEVHR+v8+fN6/fXXdeedd2rPnj0KCgqSJKWmpqpLly6qXbu2pk2bpipVqmjZsmUaNGiQPvnkE/3pT38q0X0YPny4nnrqKa1du1Zjx4696pjU1FQNGDBALVu21OzZs+Xt7a2DBw9q8+bNkqQmTZpo9uzZmj59usaNG6du3bpJkjp37uzYxpkzZ9SvXz8NGTJEDz30kGN/r+WFF16QzWbTk08+qYyMDL322muKiIhQSkqK4wz2jbiRuf2WZVm6++67tX79eo0ZM0atW7fWV199pSlTpujHH3/U/PnzncZ/8803+vTTT/XXv/5V1apV04IFCzR48GAdO3ZM1atXv+F5ArhJWQBQQS1evNiSZG3fvv2aYy5fvmxduHDBadnZs2etoKAg689//rNj2eHDhy1JVqVKlawTJ044lm/dutWSZD322GOOZb1797ZatGhhnT9/3rEsPz/f6ty5s9WwYUPHsvXr11uSrPXr1//h/fDz87PatGnjeD5jxgzrt/+Knz9/viXJOn369DW3sX37dkuStXjx4kLrevToYUmyFi5ceNV1PXr0KLRftWvXtux2u2P5smXLLEnW66+/7lgWFhZmjRw58ne3eb25jRw50goLC3M8X7FihSXJev75553G3XfffZbNZrMOHjzoWCbJ8vLyclq2a9cuS5L1xhtvFHovABUPl0kAMJq7u7vjmtf8/HxlZmbq8uXLateunXbu3Flo/KBBg1S7dm3H8w4dOqhjx4764osvJEmZmZlKTEzU/fffr5ycHP3000/66aefdObMGUVGRuqHH37Qjz/+WOL7UbVq1eveVcLf31+StHLlSuXn5xfrPby9vTV69OgbHj9ixAhVq1bN8fy+++5TrVq1HD+r0vLFF1/I3d1df/vb35yWP/7447IsS19++aXT8oiICNWvX9/xvGXLlvL19dV///vfUp0ngPKBGAZgvHfffVctW7aUj4+Pqlevrpo1a2r16tXKzs4uNLZhw4aFlt1+++06cuSIJOngwYOyLEvPPvusatas6fSYMWOGJCkjI6PE9yE3N9cpPK/0wAMPqEuXLnr44YcVFBSkIUOGaNmyZUUK49q1axfpy3JX/qxsNpsaNGjg+FmVlqNHjyokJKTQz6NJkyaO9b912223FdrGLbfcorNnz5beJAGUG1wzDMBoS5Ys0ahRozRo0CBNmTJFgYGBcnd315w5c3To0KEib68gLp944glFRkZedUyDBg3+0JyvdOLECWVnZ193u5UqVdKmTZu0fv16rV69WmvWrNHSpUt15513au3atXJ3d//d9ynKdb436lp/GCQvL++G5lQSrvU+1hVftgNQMRHDAIy2fPly1atXT59++qlTmBWcxb3SDz/8UGjZf/7zH8fdDOrVqyfp19t9RURElPyEr+L999+XpGvGdwE3Nzf17t1bvXv31rx58/Tiiy/q6aef1vr16xUREVHif7Huyp+VZVk6ePCgWrZs6Vh2yy23KCsrq9Brjx496vhZSteO5qsJCwvTunXrlJOT43R2eP/+/Y71AFCAyyQAGK3grOBvzwJu3bpVSUlJVx2/YsUKp2t+t23bpq1bt6pfv36SpMDAQPXs2VNvv/22Tp06Vej1p0+fLsnpKzExUc8995zq1q2rYcOGXXNcZmZmoWUFd8q4cOGCJKlKlSqSdNU4LY6CO28UWL58uU6dOuX4WUlS/fr1tWXLFl28eNGxbNWqVYVuwVaUufXv3195eXl68803nZbPnz9fNpvN6f0BgDPDACq8RYsWXfW+sY8++qgGDBigTz/9VH/6058UFRWlw4cPa+HChWratKlyc3MLvaZBgwbq2rWrxo8frwsXLui1115T9erVNXXqVMeY2NhYde3aVS1atNDYsWNVr149paenKykpSSdOnNCuXbuKtR9ffvml9u/fr8uXLys9PV2JiYmKj49XWFiYPv/8c/n4+FzztbNnz9amTZsUFRWlsLAwZWRk6B//+IduvfVWde3aVdKvYerv76+FCxeqWrVqqlKlijp27Ki6desWa74BAQHq2rWrRo8erfT0dL322mtq0KCB0+3fHn74YS1fvlx9+/bV/fffr0OHDmnJkiVOX2gr6twGDhyoXr166emnn9aRI0fUqlUrrV27VitXrtSkSZMKbRuA4Vx6LwsAKEUFtyS71uP48eNWfn6+9eKLL1phYWGWt7e31aZNG2vVqlWFbtdVcGu1l19+2Xr11Vet0NBQy9vb2+rWrZu1a9euQu996NAha8SIEVZwcLDl6elp1a5d2xowYIC1fPlyx5ii3lqt4OHl5WUFBwdbd911l/X666873b6swJW3VktISLDuueceKyQkxPLy8rJCQkKsoUOHWv/5z3+cXrdy5UqradOmloeHh9OtzHr06GE1a9bsqvO71q3V/v3vf1sxMTFWYGCgValSJSsqKso6evRoode/+uqrVu3atS1vb2+rS5cu1o4dOwpt83pzu/JYWZZl5eTkWI899pgVEhJieXp6Wg0bNrRefvllKz8/32mcJCs6OrrQnK51yzcAFY/NsviGAAAAAMzENcMAAAAwFjEMAAAAYxHDAAAAMBYxDAAAAGMRwwAAADAWMQwAAABj8Uc3bkB+fr5OnjypatWqlfifKwUAAMAfZ1mWcnJyFBISIje3Gz/fSwzfgJMnTyo0NNTV0wAAAMDvOH78uG699dYbHk8M34Bq1apJ+vWH6+vr6+LZAAAA4Ep2u12hoaGObrtRxPANKLg0wtfXlxgGAAAox4p6SStfoAMAAICxiGEAAAAYixgGAACAsYhhAAAAGIsYBgAAgLGIYQAAABiLGAYAAICxiGEAAAAYixgGAACAsYhhAAAAGIsYBgAAgLGIYQAAABiLGAYAAICxiGEAAAAYixgGAACAsYhhAAAAGIsYBgAAgLGIYQAAABiLGAYAAICxPFw9AQAorjrTVrt6CjDckb9HuXoKAP4gzgwDAADAWMQwAAAAjEUMAwAAwFjEMAAAAIxFDAMAAMBYxDAAAACMRQwDAADAWMQwAAAAjEUMAwAAwFjEMAAAAIxFDAMAAMBYxDAAAACMRQwDAADAWMQwAAAAjEUMAwAAwFjEMAAAAIxFDAMAAMBYxDAAAACMRQwDAADAWMQwAAAAjEUMAwAAwFjEMAAAAIxFDAMAAMBYxDAAAACMRQwDAADAWC6N4Tlz5qh9+/aqVq2aAgMDNWjQIB04cMBpzPnz5xUdHa3q1auratWqGjx4sNLT053GHDt2TFFRUapcubICAwM1ZcoUXb582WnMhg0bdMcdd8jb21sNGjRQXFxcae8eAAAAyjmXxvDGjRsVHR2tLVu2KD4+XpcuXVKfPn107tw5x5jHHntM//u//6uPP/5YGzdu1MmTJ3Xvvfc61ufl5SkqKkoXL17Ut99+q3fffVdxcXGaPn26Y8zhw4cVFRWlXr16KSUlRZMmTdLDDz+sr776qkz3FwAAAOWLzbIsy9WTKHD69GkFBgZq48aN6t69u7Kzs1WzZk19+OGHuu+++yRJ+/fvV5MmTZSUlKROnTrpyy+/1IABA3Ty5EkFBQVJkhYuXKgnn3xSp0+flpeXl5588kmtXr1ae/fudbzXkCFDlJWVpTVr1vzuvOx2u/z8/JSdnS1fX9/S2XkARVZn2mpXTwGGO/L3KFdPAcD/KW6vlatrhrOzsyVJAQEBkqTk5GRdunRJERERjjGNGzfWbbfdpqSkJElSUlKSWrRo4QhhSYqMjJTdbldqaqpjzG+3UTCmYBtXunDhgux2u9MDAAAAFU+5ieH8/HxNmjRJXbp0UfPmzSVJaWlp8vLykr+/v9PYoKAgpaWlOcb8NoQL1hesu94Yu92uX375pdBc5syZIz8/P8cjNDS0RPYRAAAA5Uu5ieHo6Gjt3btXH330kaunopiYGGVnZzsex48fd/WUAAAAUAo8XD0BSZowYYJWrVqlTZs26dZbb3UsDw4O1sWLF5WVleV0djg9PV3BwcGOMdu2bXPaXsHdJn475so7UKSnp8vX11eVKlUqNB9vb295e3uXyL4BAACg/HLpmWHLsjRhwgR99tlnSkxMVN26dZ3Wt23bVp6enkpISHAsO3DggI4dO6bw8HBJUnh4uPbs2aOMjAzHmPj4ePn6+qpp06aOMb/dRsGYgm0AAADATC49MxwdHa0PP/xQK1euVLVq1RzX+Pr5+alSpUry8/PTmDFjNHnyZAUEBMjX11cTJ05UeHi4OnXqJEnq06ePmjZtquHDh2vu3LlKS0vTM888o+joaMfZ3UceeURvvvmmpk6dqj//+c9KTEzUsmXLtHo130QHAAAwmUvPDL/11lvKzs5Wz549VatWLcdj6dKljjHz58/XgAEDNHjwYHXv3l3BwcH69NNPHevd3d21atUqubu7Kzw8XA899JBGjBih2bNnO8bUrVtXq1evVnx8vFq1aqVXX31V//znPxUZGVmm+wsAAIDypVzdZ7i84j7DQPnEfYbhatxnGCg/KsR9hgEAAICyRAwDAADAWMQwAAAAjEUMAwAAwFjEMAAAAIxFDAMAAMBYxDAAAACMRQwDAADAWMQwAAAAjEUMAwAAwFjEMAAAAIxFDAMAAMBYxDAAAACMRQwDAADAWMQwAAAAjEUMAwAAwFjEMAAAAIxFDAMAAMBYxDAAAACMRQwDAADAWMQwAAAAjEUMAwAAwFjEMAAAAIxFDAMAAMBYxDAAAACMRQwDAADAWMQwAAAAjEUMAwAAwFjEMAAAAIxFDAMAAMBYxDAAAACMRQwDAADAWMQwAAAAjEUMAwAAwFjEMAAAAIxFDAMAAMBYxDAAAACMRQwDAADAWMQwAAAAjEUMAwAAwFjEMAAAAIxFDAMAAMBYxDAAAACMRQwDAADAWMQwAAAAjEUMAwAAwFjEMAAAAIxFDAMAAMBYxDAAAACMRQwDAADAWMQwAAAAjEUMAwAAwFjEMAAAAIxFDAMAAMBYxDAAAACMRQwDAADAWMQwAAAAjEUMAwAAwFjEMAAAAIxFDAMAAMBYxDAAAACMRQwDAADAWMQwAAAAjEUMAwAAwFjEMAAAAIxFDAMAAMBYxDAAAACMRQwDAADAWMQwAAAAjEUMAwAAwFjEMAAAAIxFDAMAAMBYxDAAAACMRQwDAADAWMQwAAAAjEUMAwAAwFjEMAAAAIxFDAMAAMBYxDAAAACMRQwDAADAWMQwAAAAjEUMAwAAwFjEMAAAAIxFDAMAAMBYxDAAAACMRQwDAADAWMQwAAAAjEUMAwAAwFjEMAAAAIxFDAMAAMBYxDAAAACM5dIY3rRpkwYOHKiQkBDZbDatWLHCaf2oUaNks9mcHn379nUak5mZqWHDhsnX11f+/v4aM2aMcnNzncbs3r1b3bp1k4+Pj0JDQzV37tzS3jUAAADcBFwaw+fOnVOrVq0UGxt7zTF9+/bVqVOnHI9///vfTuuHDRum1NRUxcfHa9WqVdq0aZPGjRvnWG+329WnTx+FhYUpOTlZL7/8smbOnKl33nmn1PYLAAAANwcPV755v3791K9fv+uO8fb2VnBw8FXX7du3T2vWrNH27dvVrl07SdIbb7yh/v3765VXXlFISIg++OADXbx4UYsWLZKXl5eaNWumlJQUzZs3zymaAQAAYJ5yf83whg0bFBgYqEaNGmn8+PE6c+aMY11SUpL8/f0dISxJERERcnNz09atWx1junfvLi8vL8eYyMhIHThwQGfPnr3qe164cEF2u93pAQAAgIqnXMdw37599d577ykhIUEvvfSSNm7cqH79+ikvL0+SlJaWpsDAQKfXeHh4KCAgQGlpaY4xQUFBTmMKnheMudKcOXPk5+fneISGhpb0rgEAAKAccOllEr9nyJAhjn9u0aKFWrZsqfr162vDhg3q3bt3qb1vTEyMJk+e7Hhut9sJYgAAgAqoXJ8ZvlK9evVUo0YNHTx4UJIUHBysjIwMpzGXL19WZmam4zrj4OBgpaenO40peH6ta5G9vb3l6+vr9AAAAEDFc1PF8IkTJ3TmzBnVqlVLkhQeHq6srCwlJyc7xiQmJio/P18dO3Z0jNm0aZMuXbrkGBMfH69GjRrplltuKdsdAAAAQLni0hjOzc1VSkqKUlJSJEmHDx9WSkqKjh07ptzcXE2ZMkVbtmzRkSNHlJCQoHvuuUcNGjRQZGSkJKlJkybq27evxo4dq23btmnz5s2aMGGChgwZopCQEEnSgw8+KC8vL40ZM0apqalaunSpXn/9dafLIAAAAGAml8bwjh071KZNG7Vp00aSNHnyZLVp00bTp0+Xu7u7du/erbvvvlu33367xowZo7Zt2+rrr7+Wt7e3YxsffPCBGjdurN69e6t///7q2rWr0z2E/fz8tHbtWh0+fFht27bV448/runTp3NbNQAAAMhmWZbl6kmUd3a7XX5+fsrOzub6YaAcqTNttaunAMMd+XuUq6cA4P8Ut9duqmuGAQAAgJJEDAMAAMBYxDAAAACMRQwDAADAWMQwAAAAjEUMAwAAwFjEMAAAAIxFDAMAAMBYxDAAAACMRQwDAADAWMQwAAAAjEUMAwAAwFjEMAAAAIxFDAMAAMBYxDAAAACMRQwDAADAWMQwAAAAjEUMAwAAwFjEMAAAAIxFDAMAAMBYxDAAAACMRQwDAADAWMQwAAAAjEUMAwAAwFjEMAAAAIxFDAMAAMBYxDAAAACMRQwDAADAWMQwAAAAjEUMAwAAwFjEMAAAAIxFDAMAAMBYxDAAAACMRQwDAADAWMQwAAAAjEUMAwAAwFjEMAAAAIxFDAMAAMBYxDAAAACMRQwDAADAWMQwAAAAjEUMAwAAwFjEMAAAAIxFDAMAAMBYxDAAAACMRQwDAADAWMQwAAAAjEUMAwAAwFjEMAAAAIxFDAMAAMBYxDAAAACMRQwDAADAWMWK4Xr16unMmTOFlmdlZalevXp/eFIAAABAWShWDB85ckR5eXmFll+4cEE//vjjH54UAAAAUBY8ijL4888/d/zzV199JT8/P8fzvLw8JSQkqE6dOiU2OQAAAKA0FSmGBw0aJEmy2WwaOXKk0zpPT0/VqVNHr776aolNDgAAAChNRYrh/Px8SVLdunW1fft21ahRo1QmBQAAAJSFIsVwgcOHD5f0PAAAAIAyV6wYlqSEhAQlJCQoIyPDcca4wKJFi/7wxAAAAIDSVqwYnjVrlmbPnq127dqpVq1astlsJT0vAAAAoNQVK4YXLlyouLg4DR8+vKTnAwAAAJSZYt1n+OLFi+rcuXNJzwUAAAAoU8WK4YcfflgffvhhSc8FAAAAKFPFukzi/Pnzeuedd7Ru3Tq1bNlSnp6eTuvnzZtXIpMDAAAASlOxYnj37t1q3bq1JGnv3r1O6/gyHQAAAG4WxYrh9evXl/Q8AAAAgDJXrGuGAQAAgIqgWGeGe/Xqdd3LIRITE4s9IQAAAKCsFCuGC64XLnDp0iWlpKRo7969GjlyZEnMCwAAACh1xYrh+fPnX3X5zJkzlZub+4cmBAAAAJSVEr1m+KGHHtKiRYtKcpMAAABAqSnRGE5KSpKPj09JbhIAAAAoNcW6TOLee+91em5Zlk6dOqUdO3bo2WefLZGJAQAAAKWtWDHs5+fn9NzNzU2NGjXS7Nmz1adPnxKZGAAAAFDaihXDixcvLul5AAAAAGWuWDFcIDk5Wfv27ZMkNWvWTG3atCmRSQEAAABloVgxnJGRoSFDhmjDhg3y9/eXJGVlZalXr1766KOPVLNmzZKcIwAAAFAqinU3iYkTJyonJ0epqanKzMxUZmam9u7dK7vdrr/97W8lPUcAAACgVBTrzPCaNWu0bt06NWnSxLGsadOmio2N5Qt0AAAAuGkU68xwfn6+PD09Cy339PRUfn7+H54UAAAAUBaKFcN33nmnHn30UZ08edKx7Mcff9Rjjz2m3r17l9jkAAAAgNJUrBh+8803ZbfbVadOHdWvX1/169dX3bp1Zbfb9cYbb5T0HAEAAIBSUaxrhkNDQ7Vz506tW7dO+/fvlyQ1adJEERERJTo5AAAAoDQV6cxwYmKimjZtKrvdLpvNprvuuksTJ07UxIkT1b59ezVr1kxff/11ac0VAAAAKFFFiuHXXntNY8eOla+vb6F1fn5++stf/qJ58+aV2OQAAACA0lSkGN61a5f69u17zfV9+vRRcnLyH54UAAAAUBaKFMPp6elXvaVaAQ8PD50+ffoPTwoAAAAoC0WK4dq1a2vv3r3XXL97927VqlXrD08KAAAAKAtFiuH+/fvr2Wef1fnz5wut++WXXzRjxgwNGDCgxCYHAAAAlKYi3VrtmWee0aeffqrbb79dEyZMUKNGjSRJ+/fvV2xsrPLy8vT000+XykQBAACAklakGA4KCtK3336r8ePHKyYmRpZlSZJsNpsiIyMVGxuroKCgUpkoAAAAUNKK/Ec3wsLC9MUXX+js2bM6ePCgLMtSw4YNdcstt5TG/AAAAIBSU6w/xyxJt9xyi9q3b68OHToUO4Q3bdqkgQMHKiQkRDabTStWrHBab1mWpk+frlq1aqlSpUqKiIjQDz/84DQmMzNTw4YNk6+vr/z9/TVmzBjl5uY6jdm9e7e6desmHx8fhYaGau7cucWaLwAAACqWYsdwSTh37pxatWql2NjYq66fO3euFixYoIULF2rr1q2qUqWKIiMjnb7AN2zYMKWmpio+Pl6rVq3Spk2bNG7cOMd6u92uPn36KCwsTMnJyXr55Zc1c+ZMvfPOO6W+fwAAACjfbFbBhb8uZrPZ9Nlnn2nQoEGSfj0rHBISoscff1xPPPGEJCk7O1tBQUGKi4vTkCFDtG/fPjVt2lTbt29Xu3btJElr1qxR//79deLECYWEhOitt97S008/rbS0NHl5eUmSpk2bphUrVmj//v03NDe73S4/Pz9lZ2df9a/vAXCNOtNWu3oKMNyRv0e5egoA/k9xe82lZ4av5/Dhw0pLS1NERIRjmZ+fnzp27KikpCRJUlJSkvz9/R0hLEkRERFyc3PT1q1bHWO6d+/uCGFJioyM1IEDB3T27NmrvveFCxdkt9udHgAAAKh4ym0Mp6WlSVKhu1MEBQU51qWlpSkwMNBpvYeHhwICApzGXG0bv32PK82ZM0d+fn6OR2ho6B/fIQAAAJQ75TaGXSkmJkbZ2dmOx/Hjx109JQAAAJSCchvDwcHBkqT09HSn5enp6Y51wcHBysjIcFp/+fJlZWZmOo252jZ++x5X8vb2lq+vr9MDAAAAFU+5jeG6desqODhYCQkJjmV2u11bt25VeHi4JCk8PFxZWVlKTk52jElMTFR+fr46duzoGLNp0yZdunTJMSY+Pl6NGjXi3sgAAACGc2kM5+bmKiUlRSkpKZJ+/dJcSkqKjh07JpvNpkmTJun555/X559/rj179mjEiBEKCQlx3HGiSZMm6tu3r8aOHatt27Zp8+bNmjBhgoYMGaKQkBBJ0oMPPigvLy+NGTNGqampWrp0qV5//XVNnjzZRXsNAACA8qLIf4GuJO3YsUO9evVyPC8I1JEjRyouLk5Tp07VuXPnNG7cOGVlZalr165as2aNfHx8HK/54IMPNGHCBPXu3Vtubm4aPHiwFixY4Fjv5+entWvXKjo6Wm3btlWNGjU0ffp0p3sRAwAAwEzl5j7D5Rn3GQbKJ+4zDFfjPsNA+VHh7jMMAAAAlDZiGAAAAMYihgEAAGAsYhgAAADGIoYBAABgLGIYAAAAxiKGAQAAYCxiGAAAAMYihgEAAGAsYhgAAADGIoYBAABgLGIYAAAAxiKGAQAAYCxiGAAAAMYihgEAAGAsYhgAAADG8nD1BAAAQOmoM221q6cAwx35e5Srp/C7ODMMAAAAYxHDAAAAMBYxDAAAAGMRwwAAADAWMQwAAABjEcMAAAAwFjEMAAAAYxHDAAAAMBYxDAAAAGMRwwAAADAWMQwAAABjEcMAAAAwFjEMAAAAYxHDAAAAMBYxDAAAAGMRwwAAADAWMQwAAABjEcMAAAAwFjEMAAAAYxHDAAAAMBYxDAAAAGMRwwAAADAWMQwAAABjEcMAAAAwFjEMAAAAYxHDAAAAMBYxDAAAAGMRwwAAADAWMQwAAABjEcMAAAAwFjEMAAAAYxHDAAAAMBYxDAAAAGMRwwAAADAWMQwAAABjEcMAAAAwFjEMAAAAYxHDAAAAMBYxDAAAAGMRwwAAADAWMQwAAABjEcMAAAAwFjEMAAAAYxHDAAAAMBYxDAAAAGMRwwAAADAWMQwAAABjEcMAAAAwFjEMAAAAYxHDAAAAMBYxDAAAAGMRwwAAADAWMQwAAABjEcMAAAAwFjEMAAAAYxHDAAAAMBYxDAAAAGMRwwAAADAWMQwAAABjEcMAAAAwFjEMAAAAYxHDAAAAMBYxDAAAAGMRwwAAADAWMQwAAABjEcMAAAAwFjEMAAAAYxHDAAAAMBYxDAAAAGMRwwAAADAWMQwAAABjEcMAAAAwFjEMAAAAYxHDAAAAMFa5juGZM2fKZrM5PRo3buxYf/78eUVHR6t69eqqWrWqBg8erPT0dKdtHDt2TFFRUapcubICAwM1ZcoUXb58uax3BQAAAOWQh6sn8HuaNWumdevWOZ57ePz/KT/22GNavXq1Pv74Y/n5+WnChAm69957tXnzZklSXl6eoqKiFBwcrG+//VanTp3SiBEj5OnpqRdffLHM9wUAAADlS7mPYQ8PDwUHBxdanp2drX/961/68MMPdeedd0qSFi9erCZNmmjLli3q1KmT1q5dq++//17r1q1TUFCQWrdureeee05PPvmkZs6cKS8vr7LeHQAAAJQj5foyCUn64YcfFBISonr16mnYsGE6duyYJCk5OVmXLl1SRESEY2zjxo112223KSkpSZKUlJSkFi1aKCgoyDEmMjJSdrtdqamp13zPCxcuyG63Oz0AAABQ8ZTrGO7YsaPi4uK0Zs0avfXWWzp8+LC6deumnJwcpaWlycvLS/7+/k6vCQoKUlpamiQpLS3NKYQL1hesu5Y5c+bIz8/P8QgNDS3ZHQMAAEC5UK4vk+jXr5/jn1u2bKmOHTsqLCxMy5YtU6VKlUrtfWNiYjR58mTHc7vdThADAABUQOX6zPCV/P39dfvtt+vgwYMKDg7WxYsXlZWV5TQmPT3dcY1xcHBwobtLFDy/2nXIBby9veXr6+v0AAAAQMVzU8Vwbm6uDh06pFq1aqlt27by9PRUQkKCY/2BAwd07NgxhYeHS5LCw8O1Z88eZWRkOMbEx8fL19dXTZs2LfP5AwAAoHwp15dJPPHEExo4cKDCwsJ08uRJzZgxQ+7u7ho6dKj8/Pw0ZswYTZ48WQEBAfL19dXEiRMVHh6uTp06SZL69Omjpk2bavjw4Zo7d67S0tL0zDPPKDo6Wt7e3i7eOwAAALhauY7hEydOaOjQoTpz5oxq1qyprl27asuWLapZs6Ykaf78+XJzc9PgwYN14cIFRUZG6h//+Ifj9e7u7lq1apXGjx+v8PBwValSRSNHjtTs2bNdtUsAAAAoR8p1DH/00UfXXe/j46PY2FjFxsZec0xYWJi++OKLkp4aAAAAKoCb6pphAAAAoCQRwwAAADAWMQwAAABjEcMAAAAwFjEMAAAAYxHDAAAAMBYxDAAAAGMRwwAAADAWMQwAAABjEcMAAAAwVrn+c8ymqzNttaunAMMd+XuUq6cAAECp4swwAAAAjEUMAwAAwFjEMAAAAIxFDAMAAMBYxDAAAACMRQwDAADAWMQwAAAAjEUMAwAAwFjEMAAAAIxFDAMAAMBYxDAAAACMRQwDAADAWMQwAAAAjEUMAwAAwFjEMAAAAIxFDAMAAMBYxDAAAACMRQwDAADAWMQwAAAAjEUMAwAAwFjEMAAAAIxFDAMAAMBYxDAAAACMRQwDAADAWMQwAAAAjEUMAwAAwFjEMAAAAIxFDAMAAMBYxDAAAACMRQwDAADAWMQwAAAAjEUMAwAAwFjEMAAAAIxFDAMAAMBYxDAAAACMRQwDAADAWMQwAAAAjEUMAwAAwFjEMAAAAIxFDAMAAMBYxDAAAACMRQwDAADAWMQwAAAAjEUMAwAAwFjEMAAAAIxFDAMAAMBYxDAAAACMRQwDAADAWMQwAAAAjEUMAwAAwFjEMAAAAIxFDAMAAMBYxDAAAACMRQwDAADAWMQwAAAAjEUMAwAAwFjEMAAAAIxFDAMAAMBYxDAAAACMRQwDAADAWMQwAAAAjEUMAwAAwFjEMAAAAIxFDAMAAMBYxDAAAACMRQwDAADAWMQwAAAAjEUMAwAAwFjEMAAAAIxFDAMAAMBYxDAAAACMRQwDAADAWMQwAAAAjEUMAwAAwFjEMAAAAIxFDAMAAMBYxDAAAACMRQwDAADAWMQwAAAAjEUMAwAAwFjEMAAAAIxlVAzHxsaqTp068vHxUceOHbVt2zZXTwkAAAAuZEwML126VJMnT9aMGTO0c+dOtWrVSpGRkcrIyHD11AAAAOAixsTwvHnzNHbsWI0ePVpNmzbVwoULVblyZS1atMjVUwMAAICLeLh6AmXh4sWLSk5OVkxMjGOZm5ubIiIilJSUVGj8hQsXdOHCBcfz7OxsSZLdbi/9yf5G/oWfy/T9gCuV9f/mi4rPCFyNzwhwfWX5GSl4L8uyivQ6I2L4p59+Ul5enoKCgpyWBwUFaf/+/YXGz5kzR7NmzSq0PDQ0tNTmCJRHfq+5egZA+cZnBLg+V3xGcnJy5Ofnd8PjjYjhooqJidHkyZMdz/Pz85WZmanq1avLZrMVGm+32xUaGqrjx4/L19e3LKeKa+CYlD8ck/KF41H+cEzKH45J+fJ7x8OyLOXk5CgkJKRI2zUihmvUqCF3d3elp6c7LU9PT1dwcHCh8d7e3vL29nZa5u/v/7vv4+vry4elnOGYlD8ck/KF41H+cEzKH45J+XK941GUM8IFjPgCnZeXl9q2bauEhATHsvz8fCUkJCg8PNyFMwMAAIArGXFmWJImT56skSNHql27durQoYNee+01nTt3TqNHj3b11AAAAOAixsTwAw88oNOnT2v69OlKS0tT69attWbNmkJfqisOb29vzZgxo9ClFXAdjkn5wzEpXzge5Q/HpPzhmJQvpXU8bFZR7z8BAAAAVBBGXDMMAAAAXA0xDAAAAGMRwwAAADAWMQwAAABjEcPFlJmZqWHDhsnX11f+/v4aM2aMcnNzr/uanj17ymazOT0eeeSRMppxxRMbG6s6derIx8dHHTt21LZt2647/uOPP1bjxo3l4+OjFi1a6IsvviijmZqjKMckLi6u0OfBx8enDGdbsW3atEkDBw5USEiIbDabVqxY8buv2bBhg+644w55e3urQYMGiouLK/V5mqSox2TDhg2FPiM2m01paWllM+EKbs6cOWrfvr2qVaumwMBADRo0SAcOHPjd1/G7pHQU53iU1O8RYriYhg0bptTUVMXHx2vVqlXatGmTxo0b97uvGzt2rE6dOuV4zJ07twxmW/EsXbpUkydP1owZM7Rz5061atVKkZGRysjIuOr4b7/9VkOHDtWYMWP03XffadCgQRo0aJD27t1bxjOvuIp6TKRf/4rQbz8PR48eLcMZV2znzp1Tq1atFBsbe0PjDx8+rKioKPXq1UspKSmaNGmSHn74YX311VelPFNzFPWYFDhw4IDT5yQwMLCUZmiWjRs3Kjo6Wlu2bFF8fLwuXbqkPn366Ny5c9d8Db9LSk9xjodUQr9HLBTZ999/b0mytm/f7lj25ZdfWjabzfrxxx+v+boePXpYjz76aBnMsOLr0KGDFR0d7Xiel5dnhYSEWHPmzLnq+Pvvv9+KiopyWtaxY0frL3/5S6nO0yRFPSaLFy+2/Pz8ymh2ZpNkffbZZ9cdM3XqVKtZs2ZOyx544AErMjKyFGdmrhs5JuvXr7ckWWfPni2TOZkuIyPDkmRt3LjxmmP4XVJ2buR4lNTvEc4MF0NSUpL8/f3Vrl07x7KIiAi5ublp69at133tBx98oBo1aqh58+aKiYnRzz//XNrTrXAuXryo5ORkRUREOJa5ubkpIiJCSUlJV31NUlKS03hJioyMvOZ4FE1xjokk5ebmKiwsTKGhobrnnnuUmppaFtPFVfAZKb9at26tWrVq6a677tLmzZtdPZ0KKzs7W5IUEBBwzTF8TsrOjRwPqWR+jxDDxZCWllboP1N5eHgoICDgutdyPfjgg1qyZInWr1+vmJgYvf/++3rooYdKe7oVzk8//aS8vLxCfz0wKCjomj//tLS0Io1H0RTnmDRq1EiLFi3SypUrtWTJEuXn56tz5846ceJEWUwZV7jWZ8Rut+uXX35x0azMVqtWLS1cuFCffPKJPvnkE4WGhqpnz57auXOnq6dW4eTn52vSpEnq0qWLmjdvfs1x/C4pGzd6PErq94gxf475RkybNk0vvfTSdcfs27ev2Nv/7TXFLVq0UK1atdS7d28dOnRI9evXL/Z2gZtReHi4wsPDHc87d+6sJk2a6O2339Zzzz3nwpkB5UOjRo3UqFEjx/POnTvr0KFDmj9/vt5//30XzqziiY6O1t69e/XNN9+4eirQjR+Pkvo9Qgz/xuOPP65Ro0Zdd0y9evUUHBxc6EtBly9fVmZmpoKDg2/4/Tp27ChJOnjwIDFcBDVq1JC7u7vS09Odlqenp1/z5x8cHFyk8Sia4hyTK3l6eqpNmzY6ePBgaUwRv+NanxFfX19VqlTJRbPClTp06ECwlbAJEyY4vgh/6623Xncsv0tKX1GOx5WK+3uEyyR+o2bNmmrcuPF1H15eXgoPD1dWVpaSk5Mdr01MTFR+fr4jcG9ESkqKpF//UxhunJeXl9q2bauEhATHsvz8fCUkJDj9P8TfCg8PdxovSfHx8dccj6IpzjG5Ul5envbs2cPnwUX4jNwcUlJS+IyUEMuyNGHCBH322WdKTExU3bp1f/c1fE5KT3GOx5WK/XvkD38Fz1B9+/a12rRpY23dutX65ptvrIYNG1pDhw51rD9x4oTVqFEja+vWrZZlWdbBgwet2bNnWzt27LAOHz5srVy50qpXr57VvXt3V+3CTe2jjz6yvL29rbi4OOv777+3xo0bZ/n7+1tpaWmWZVnW8OHDrWnTpjnGb9682fLw8LBeeeUVa9++fdaMGTMsT09Pa8+ePa7ahQqnqMdk1qxZ1ldffWUdOnTISk5OtoYMGWL5+PhYqamprtqFCiUnJ8f67rvvrO+++86SZM2bN8/67rvvrKNHj1qWZVnTpk2zhg8f7hj/3//+16pcubI1ZcoUa9++fVZsbKzl7u5urVmzxlW7UOEU9ZjMnz/fWrFihfXDDz9Ye/bssR599FHLzc3NWrdunat2oUIZP3685efnZ23YsME6deqU4/Hzzz87xvC7pOwU53iU1O8RYriYzpw5Yw0dOtSqWrWq5evra40ePdrKyclxrD98+LAlyVq/fr1lWZZ17Ngxq3v37lZAQIDl7e1tNWjQwJoyZYqVnZ3toj24+b3xxhvWbbfdZnl5eVkdOnSwtmzZ4ljXo0cPa+TIkU7jly1bZt1+++2Wl5eX1axZM2v16tVlPOOKryjHZNKkSY6xQUFBVv/+/a2dO3e6YNYVU8Ftua58FByDkSNHWj169Cj0mtatW1teXl5WvXr1rMWLF5f5vCuyoh6Tl156yapfv77l4+NjBQQEWD179rQSExNdM/kK6GrHQpLT/+75XVJ2inM8Sur3iO3/JgAAAAAYh2uGAQAAYCxiGAAAAMYihgEAAGAsYhgAAADGIoYBAABgLGIYAAAAxiKGAQAAYCxiGAAAAMYihgHAMHFxcfL39//D27HZbFqxYsUf3g4AuBIxDAA3oVGjRmnQoEGungYA3PSIYQAAABiLGAaACmbevHlq0aKFqlSpotDQUP31r39Vbm5uoXErVqxQw4YN5ePjo8jISB0/ftxp/cqVK3XHHXfIx8dH9erV06xZs3T58uWy2g0AKBPEMABUMG5ublqwYIFSU1P17rvvKjExUVOnTnUa8/PPP+uFF17Qe++9p82bNysrK0tDhgxxrP/66681YsQIPfroo/r+++/19ttvKy4uTi+88EJZ7w4AlCqbZVmWqycBACiaUaNGKSsr64a+wLZ8+XI98sgj+umnnyT9+gW60aNHa8uWLerYsaMkaf/+/WrSpIm2bt2qDh06KCIiQr1791ZMTIxjO0uWLNHUqVN18uRJSb9+ge6zzz7j2mUANzUPV08AAFCy1q1bpzlz5mj//v2y2+26fPmyzp8/r59//lmVK1eWJHl4eKh9+/aO1zRu3Fj+/v7at2+fOnTooF27dmnz5s1OZ4Lz8vIKbQcAbnbEMABUIEeOHNGAAQM0fvx4vfDCCwoICNA333yjMWPG6OLFizccsbm5uZo1a5buvffeQut8fHxKetoA4DLEMABUIMnJycrPz9err74qN7dfvxaybNmyQuMuX76sHTt2qEOHDpKkAwcOKCsrS02aNJEk3XHHHTpw4IAaNGhQdpMHABcghgHgJpWdna2UlBSnZTVq1NClS5f0xhtvaODAgdq8ebMWLlxY6LWenp6aOHGiFixYIA8PD02YMEGdOnVyxPH06dM1YMAA3Xbbbbrvvvvk5uamXbt2ae/evXr++efLYvcAoExwNwkAuElt2LBBbdq0cXq8//77mjdvnl566SU1b95cH3zwgebMmVPotZUrV9aTTz6pBx98UF26dFHVqlW1dOlSx/rIyEitWrVKa9euVfv27dWpUyfNnz9fYWFhZbmLAFDquJsEAAAAjMWZYQAAABiLGAYAAICxiGEAAAAYixgGAACAsYhhAAAAGIsYBgAAgLGIYQAAABiLGAYAAICxiGEAAAAYixgGAACAsYhhAAAAGOv/AR/5CMTcZoGWAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Count the occurrences of each label\n",
    "label_counts = headlines['label'].value_counts()\n",
    "\n",
    "# Create a bar plot of the label distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(label_counts.index, label_counts.values)\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Label Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Less is more when training a transformer model, such as RoBERTa, for text classification. The basic concept behind transformers is to learn optimal representations of language by understanding the contexts, nuances, and intricacies present in the text. This is achieved by representing text as a sequence of tokens and using an attention mechanism to learn the effect of each token on the others in the sequence. Training the model on unprocessed text preserves the semantic richness of the language, which would otherwise be parsed out through the removal of special characters and stop words, stemming, lemmatization, etc. While text cleaning is warranted for simpler models that benefit from uniformity, dimensionality reduction, and noise removal, (see Tf-IDF, count vectorization, etc.) opting out of text processing is advantageous here since the news headlines are short enough to meet the 512-token input limit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check: Do our Inputs Meet the 512 Token Limit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([1222, 1654,  729, 2139, 2996,  319, 2708, 3280, 2196, 1903,  975, 2195,\n",
       "       2733, 1871, 1528, 1699, 3201,  959,  958,  850,   48, 1525, 2119, 1653,\n",
       "       1242,  677,  621,  722,  359, 1232, 1083,  330, 2056, 2130,   59,  905,\n",
       "       1077, 3222, 1828, 1876, 1385, 1387, 3202, 1633, 2762,  114, 1199, 3197,\n",
       "         60, 3436],\n",
       "      dtype='int64')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What are the indices of the 10 longest notes?\n",
    "# These are the longest character lengths, not necessarily the longest token lengths\n",
    "ten_longest_inputs_idx = headlines['sentence'].apply(lambda x: len(x)).sort_values(ascending=False).index[:50]\n",
    "\n",
    "ten_longest_inputs_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check how many tokens these are... Might not need all 512 tokens\n",
    "ten_longest_inputs = headlines['sentence'].iloc[ten_longest_inputs_idx].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer\n"
     ]
    }
   ],
   "source": [
    "print('Loading tokenizer')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"soleimanian/financial-roberta-large-sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest sequence is 133 tokens\n"
     ]
    }
   ],
   "source": [
    "token_lengths = []\n",
    "max_tokens = 0\n",
    "for input in ten_longest_inputs:\n",
    "    encodings = tokenizer(input,\n",
    "                            add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length=512,\n",
    "                            truncation=True,\n",
    "                            padding='do_not_pad')\n",
    "    input_ids = encodings.input_ids\n",
    "    max_tokens = max(max_tokens, len(input_ids))\n",
    "\n",
    "print(f'The longest sequence is {max_tokens} tokens')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Positive': 2, 'Neutral': 1, 'Negative': 0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id = {'Positive': 2,\n",
    "            'Neutral': 1,\n",
    "            'Negative': 0\n",
    "            }\n",
    "id2label = {0: 'Negative',\n",
    "            1:'Neutral',\n",
    "            2: 'Positive'}\n",
    "    \n",
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Negative', 1: 'Neutral', 2: 'Positive'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>For the last quarter of 2010 , Componenta 's n...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In the third quarter of 2010 , net sales incre...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Operating profit rose to EUR 13.1 mn from EUR ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0  According to Gran , the company has no plans t...      1\n",
       "1  With the new production plant the company woul...      2\n",
       "2  For the last quarter of 2010 , Componenta 's n...      2\n",
       "3  In the third quarter of 2010 , net sales incre...      2\n",
       "4  Operating profit rose to EUR 13.1 mn from EUR ...      2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headlines.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Train/Test Splits\n",
    "\n",
    "Use Stratified Shuffle Split to ensure equal class distribution among splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = headlines[['sentence']].values\n",
    "y= headlines.label.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the split sizes\n",
    "split_sizes = [0.8, 0.1, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the splitter with 2 splits\n",
    "splitter = StratifiedShuffleSplit(n_splits=2, test_size=split_sizes[-1], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into a train-dev set and a test set\n",
    "train_dev_indices, test_indices = next(splitter.split(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the train-dev data and the test data\n",
    "X_train_dev, y_train_dev = X[train_dev_indices], y[train_dev_indices]\n",
    "X_test, y_test = X[test_indices], y[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sizes of the train and dev sets\n",
    "train_size = int(split_sizes[0] * len(X_train_dev))\n",
    "dev_size = int(split_sizes[1] * len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the train-dev set into a train set and a dev set\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=dev_size, random_state=42)\n",
    "train_indices, dev_indices = next(splitter.split(X_train_dev, y_train_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the train data and the dev data\n",
    "X_train, y_train = X_train_dev[train_indices], y_train_dev[train_indices]\n",
    "X_dev, y_dev = X_train_dev[dev_indices], y_train_dev[dev_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Profit for the period was EUR 10.9 mn , down from EUR 14.3 mn in 2009 .'],\n",
       "       ['Neste Shipping is the most likely to remain Finnish as the oil sector and its transports are significant for emergency supply .'],\n",
       "       ['Finnish lifting equipment maker Konecranes Oyj said on July 30 , 2008 that its net profit rose to 71.2 mln euro ( $ 111.1 mln ) for the first half of 2008 from 57.1 mln euro ( $ 89.1 mln ) for the same period of 2007 .'],\n",
       "       ...,\n",
       "       ['Deliveries will start in the second half of 2007 and the start-up of the mill is scheduled for 2008 .'],\n",
       "       [\"Founded in 1923 , Finnair is one of the world 's oldest airlines and flies to 60 destinations with a fleet of 63 aircraft , employing 9,500 people .\"],\n",
       "       ['Country : ; Germany Sector : Construction-Real Estate ; Machinery-Engineering Target : Caverion GmbH Buyer : YIT Oyj Deal size in USD : 90.3 m Type : Corporate acquisition Status : Agreed']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2762, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(345, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(346, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the lists of sentences and their labels.\n",
    "train_sentences = [narr[0] for narr in X_train] \n",
    "train_labels = y_train\n",
    "dev_sentences = [narr[0] for narr in X_dev]\n",
    "dev_labels = y_dev\n",
    "test_sentences = [narr[0] for narr in X_test]\n",
    "test_labels = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization abstracts the news headlines into a structured form that RoBERTa can interpret. This involves breaking the text into smaller units, inserting [CLS] and [SEP] tokens to mark the beginning and end of sentences, padding shorter inputs with [PAD] tokens to a uniform length (133 length as discovered earlier), and truncating longer ones. Attention masks are also created to distinguish between actual inputs and padding. The breakdown and structuring of text in this manner enable the model to effectively understand unseen words, allocate attention appropriately, and capture the contextual relationships within the text, allowing it to make more accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encodings(sentences):\n",
    "\n",
    "    sentence_dataset = Dataset.from_dict({'sentences':sentences } )\n",
    "    encodings = sentence_dataset.map(lambda x: tokenizer(\n",
    "                                                                x['sentences'], \n",
    "                                                                add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
    "                                                                max_length=max_tokens, # Replace this part with max_tokens for BigBird\n",
    "                                                                truncation=True,\n",
    "                                                                padding='max_length',\n",
    "                                                                ),\n",
    "                                            batched=True,\n",
    "                                            )\n",
    "\n",
    "    return encodings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa21480cd7bf432b891257f94a1d36d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2762 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_encodings = get_encodings(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = train_encodings[:]['input_ids']\n",
    "train_attention = train_encodings[:]['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92f0ea467ce947ef836cb8a2c4451b7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/345 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dev_encodings = get_encodings(dev_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_inputs = dev_encodings[:]['input_ids']\n",
    "dev_attention = dev_encodings[:]['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d914c111594997b77a8c64331190c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/346 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_encodings = get_encodings(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = test_encodings[:]['input_ids']\n",
    "test_attention = test_encodings[:]['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all inputs and labels into torch tensors, the required datatype \n",
    "# for our model\n",
    "#Inputs\n",
    "train_inputs = torch.tensor(train_inputs, dtype=torch.int32)\n",
    "dev_inputs = torch.tensor(dev_inputs, dtype=torch.int32)\n",
    "test_inputs = torch.tensor(test_inputs, dtype=torch.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Labels\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "dev_labels = torch.tensor(dev_labels, dtype=torch.long)\n",
    "test_labels = torch.tensor(test_labels, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Masks\n",
    "train_masks = torch.tensor(train_attention, dtype=torch.int32)\n",
    "dev_masks = torch.tensor(dev_attention, dtype=torch.int32)\n",
    "test_masks = torch.tensor(test_attention, dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the input ids, masks, etc. into separate datasets\n",
    "\n",
    "train_dataset = Dataset.from_dict({'input_ids':train_inputs,\n",
    "                                    'attention_mask': train_masks,\n",
    "                                    'labels': train_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataset =  Dataset.from_dict({'input_ids': dev_inputs,\n",
    "                                 'attention_mask': dev_masks,\n",
    "                                 'labels':dev_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset =  Dataset.from_dict({'input_ids':test_inputs,\n",
    "                                    'attention_mask':test_masks,\n",
    "                                    'labels':test_labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RoBERTa (Robustly Optimized BERT Approach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RoBERTa was chosen for this project to harness its advanced contextual understanding and exceptional performance in text classification tasks. RoBERTa is a high-performing transformer model developed by Facebook AI, excelling in numerous NLP tasks by leveraging attention mechanisms to understand the intricacies of language. Attention mechanisms enable the model to process and interpret the contextual relations and nuances between different parts of the input text. RoBERTa surpasses its predecessor, BERT, by using refined training techniques, larger datasets, and architectural improvements, which include the elimination of BERT's Next Sentence Prediction objective and dynamic adjustment of input token masking. This optimized training and enhanced contextual understanding make RoBERTa an excellent choice for this text classsification task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leveraging a Pre-Trained Model for Transfer Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Financial RoBERTa provides an opportunity for transfer learning, as it already acquired acquired an understanding of financial texts through fine-tuning on extensive financial documents like 10Ks and earnings call transcripts (important to note not the dataset used for this task). Using this specialized, pre-trained model facilitates more powerful predictions by effectively increasing the knowledge base of the text documents the model has to learn a language representation from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"soleimanian/financial-roberta-large-sentiment\", \n",
    "    num_labels = len(label2id),\n",
    "    # attention_probs_dropout_prob=0.1,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    device_map='auto'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1421459500\n"
     ]
    }
   ],
   "source": [
    "print(model.get_memory_footprint())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=1024, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics: Accuracy, Macro/Weighted F1, Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy and other metrics of our predictions vs labels\n",
    "def compute_metrics_multiclass(eval_pred):\n",
    "    # Labels give the ground truth for each *sentence* (num_classes, 1)\n",
    "    # Preds give the unnormalized probabilities for each class in a sentence (num_sentences, num_classes)\n",
    "    preds, labels = eval_pred\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten() # Get the index of the most probable classes (Already mapped index=class) and flatten to give one pred/sentence\n",
    "    labels_flat = labels.flatten()\n",
    "    print(\"Predicted Sample: \")\n",
    "    print(preds_flat[:10])\n",
    "    print(\"Actual Sample: \")\n",
    "    print(labels_flat[:10])\n",
    "    # Convert logits to float32 before applying softmax (flexibility for fp16 training)\n",
    "    preds_float32 = torch.tensor(preds, dtype=torch.float32)\n",
    "\n",
    "    # Convert logits to probabilities \n",
    "    preds_prob = torch.nn.functional.softmax(torch.tensor(preds_float32), dim=-1)\n",
    "\n",
    "    # Calculate the accuracy of the top 1 predictions\n",
    "    acc = accuracy_score(labels_flat, preds_flat)\n",
    "\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(labels_flat, preds_flat, average='weighted')\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(labels_flat, preds_flat, average='macro')\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'precision_weighted': precision_weighted,\n",
    "        'precision_macro': precision_macro,\n",
    "        'recall_weighted': recall_weighted,\n",
    "        'recall_macro': recall_macro,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'f1_macro': f1_macro\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In response to past experiences with extensive memory usage and slow computational speed during model training, adjustments were made to optimize resource efficiency. Mixed precision training (fp16) was employed, reducing memory requirements and accelerating computations. Concurrently, the 8BitAdam optimizer was used to compress model weights from 32 bits to 8 bits, conserving memory while preserving performance. Additionally, early stopping was implemented to prevent unnecessary computations and overfitting, contributing to a more efficient and dependable training process, essential for managing large datasets and advanced deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aum Thaker\\Desktop\\VSC\\jupyter\\myvenv\\Lib\\site-packages\\transformers\\training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Set threads==num cores on the machine\n",
    "torch.set_num_threads(mp.cpu_count())\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"Sentiment_Analysis_1stPass\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    do_eval=True,\n",
    "    eval_strategy='epochs',\n",
    "    do_predict=True,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=15,        # Leave extra epochs in case learning brakes out of unoptimal convergence\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    learning_rate=2e-6,\n",
    "    warmup_ratio = 0.4,  \n",
    "    fp16=True,  # gradients are computed in half precision & converted back to 32bit for the optimization step\n",
    "    fp16_opt_level='O2',\n",
    "    disable_tqdm=False,\n",
    "    dataloader_pin_memory=True,\n",
    "    logging_dir='./logs',\n",
    "    save_total_limit=1,\n",
    "    # metric_for_best_model=\"f1_macro\",\n",
    "    greater_is_better=False,  # Want smaller loss\n",
    "    # load_best_model_at_end=True,\n",
    "    resume_from_checkpoint=False,\n",
    "    remove_unused_columns=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\n",
    "        'query',\n",
    "        'key',\n",
    "        'value'\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"SEQ_CLS\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 5771267 || all params: 361134086 || trainable%: 1.5980953401335813\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aum Thaker\\Desktop\\VSC\\jupyter\\myvenv\\Lib\\site-packages\\accelerate\\accelerator.py:482: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin,)\n",
    "\n",
    "#Note: Not sure if this is gonna work but wont have any adverse effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = accelerator.prepare_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 5771267 || all params: 361134086 || trainable%: 1.5980953401335813\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     if \"lora\" in name:\n",
    "#         param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param in model.parameters():\n",
    "\n",
    "#     if not param.requires_grad:\n",
    "#         print(f\"Parameter {param} is frozen (requires_grad=False)\")\n",
    "        \n",
    "#     param.requires_grad = True  # Ensure all parameters require gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    # optimizers=(adam_bnb_optim, None), #Uncomment accordinggly if you want 8 bit adam\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    compute_metrics=compute_metrics_multiclass,\n",
    "    # callbacks=[EarlyStoppingCallback(early_stopping_patience=5, early_stopping_threshold=0.01)],\n",
    "    tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   1377 MiB |   1377 MiB |   1377 MiB |    512 B   |\n",
      "|       from large pool |   1358 MiB |   1358 MiB |   1358 MiB |      0 B   |\n",
      "|       from small pool |     19 MiB |     19 MiB |     19 MiB |    512 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   1377 MiB |   1377 MiB |   1377 MiB |    512 B   |\n",
      "|       from large pool |   1358 MiB |   1358 MiB |   1358 MiB |      0 B   |\n",
      "|       from small pool |     19 MiB |     19 MiB |     19 MiB |    512 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |   1377 MiB |   1377 MiB |   1377 MiB |      8 B   |\n",
      "|       from large pool |   1358 MiB |   1358 MiB |   1358 MiB |      0 B   |\n",
      "|       from small pool |     19 MiB |     19 MiB |     19 MiB |      8 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   1386 MiB |   1386 MiB |   1388 MiB |   2048 KiB |\n",
      "|       from large pool |   1366 MiB |   1366 MiB |   1366 MiB |      0 KiB |\n",
      "|       from small pool |     20 MiB |     20 MiB |     22 MiB |   2048 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   8574 KiB |  22160 KiB | 418319 KiB | 409745 KiB |\n",
      "|       from large pool |   7828 KiB |  20116 KiB | 396948 KiB | 389120 KiB |\n",
      "|       from small pool |    746 KiB |   2047 KiB |  21371 KiB |  20625 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     543    |     543    |     544    |       1    |\n",
      "|       from large pool |     148    |     148    |     148    |       0    |\n",
      "|       from small pool |     395    |     395    |     396    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     543    |     543    |     544    |       1    |\n",
      "|       from large pool |     148    |     148    |     148    |       0    |\n",
      "|       from small pool |     395    |     395    |     396    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      78    |      78    |      79    |       1    |\n",
      "|       from large pool |      68    |      68    |      68    |       0    |\n",
      "|       from small pool |      10    |      10    |      11    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       5    |       5    |      36    |      31    |\n",
      "|       from large pool |       3    |       3    |      25    |      22    |\n",
      "|       from small pool |       2    |       2    |      11    |       9    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: 225100006 (225100006-indian-institute-of-technology-dharwad). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Aum Thaker\\Downloads\\wandb\\run-20240919_234511-2tsmd5ju</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/225100006-indian-institute-of-technology-dharwad/huggingface/runs/2tsmd5ju' target=\"_blank\">Sentiment_Analysis_1stPass</a></strong> to <a href='https://wandb.ai/225100006-indian-institute-of-technology-dharwad/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/225100006-indian-institute-of-technology-dharwad/huggingface' target=\"_blank\">https://wandb.ai/225100006-indian-institute-of-technology-dharwad/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/225100006-indian-institute-of-technology-dharwad/huggingface/runs/2tsmd5ju' target=\"_blank\">https://wandb.ai/225100006-indian-institute-of-technology-dharwad/huggingface/runs/2tsmd5ju</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd476f37254a4fde9fd7dcc2f4827653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aum Thaker\\Desktop\\VSC\\jupyter\\myvenv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\Aum Thaker\\Desktop\\VSC\\jupyter\\myvenv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0e53ab78e5d4bee89f3d8322d24b223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sample: \n",
      "[2 2 2 2 2 0 2 1 0 1]\n",
      "Actual Sample: \n",
      "[2 2 1 2 2 0 1 1 0 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aum Thaker\\AppData\\Local\\Temp\\ipykernel_3088\\76004914.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  preds_prob = torch.nn.functional.softmax(torch.tensor(preds_float32), dim=-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6778643727302551, 'eval_accuracy': 0.8521739130434782, 'eval_precision_weighted': 0.8856732911176686, 'eval_precision_macro': 0.818207511355893, 'eval_recall_weighted': 0.8521739130434782, 'eval_recall_macro': 0.9098065002191853, 'eval_f1_weighted': 0.854412409791293, 'eval_f1_macro': 0.848550649166732, 'eval_runtime': 3.7784, 'eval_samples_per_second': 91.309, 'eval_steps_per_second': 2.911, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aum Thaker\\Desktop\\VSC\\jupyter\\myvenv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\Aum Thaker\\Desktop\\VSC\\jupyter\\myvenv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1327a476cea243009ccb89094753e1ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sample: \n",
      "[2 2 2 2 2 0 2 1 0 1]\n",
      "Actual Sample: \n",
      "[2 2 1 2 2 0 1 1 0 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aum Thaker\\AppData\\Local\\Temp\\ipykernel_3088\\76004914.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  preds_prob = torch.nn.functional.softmax(torch.tensor(preds_float32), dim=-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6645127534866333, 'eval_accuracy': 0.8521739130434782, 'eval_precision_weighted': 0.8856732911176686, 'eval_precision_macro': 0.818207511355893, 'eval_recall_weighted': 0.8521739130434782, 'eval_recall_macro': 0.9098065002191853, 'eval_f1_weighted': 0.854412409791293, 'eval_f1_macro': 0.848550649166732, 'eval_runtime': 3.7565, 'eval_samples_per_second': 91.842, 'eval_steps_per_second': 2.928, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aum Thaker\\Desktop\\VSC\\jupyter\\myvenv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\Aum Thaker\\Desktop\\VSC\\jupyter\\myvenv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8227a869af4c4c1a88fcec73f74f0b5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sample: \n",
      "[2 2 2 2 2 0 2 1 0 1]\n",
      "Actual Sample: \n",
      "[2 2 1 2 2 0 1 1 0 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aum Thaker\\AppData\\Local\\Temp\\ipykernel_3088\\76004914.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  preds_prob = torch.nn.functional.softmax(torch.tensor(preds_float32), dim=-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6383159160614014, 'eval_accuracy': 0.8521739130434782, 'eval_precision_weighted': 0.8856732911176686, 'eval_precision_macro': 0.818207511355893, 'eval_recall_weighted': 0.8521739130434782, 'eval_recall_macro': 0.9098065002191853, 'eval_f1_weighted': 0.854412409791293, 'eval_f1_macro': 0.848550649166732, 'eval_runtime': 3.7708, 'eval_samples_per_second': 91.492, 'eval_steps_per_second': 2.917, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aum Thaker\\Desktop\\VSC\\jupyter\\myvenv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\Aum Thaker\\Desktop\\VSC\\jupyter\\myvenv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45d7795f2cff4150970df2ac7bf2c927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sample: \n",
      "[2 2 2 2 2 0 2 1 0 1]\n",
      "Actual Sample: \n",
      "[2 2 1 2 2 0 1 1 0 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aum Thaker\\AppData\\Local\\Temp\\ipykernel_3088\\76004914.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  preds_prob = torch.nn.functional.softmax(torch.tensor(preds_float32), dim=-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6006162762641907, 'eval_accuracy': 0.855072463768116, 'eval_precision_weighted': 0.887258416921286, 'eval_precision_macro': 0.8202068216328465, 'eval_recall_weighted': 0.855072463768116, 'eval_recall_macro': 0.9113641326179391, 'eval_f1_weighted': 0.8572558661145618, 'eval_f1_macro': 0.8508432539682539, 'eval_runtime': 3.8726, 'eval_samples_per_second': 89.088, 'eval_steps_per_second': 2.841, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aum Thaker\\Desktop\\VSC\\jupyter\\myvenv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\Aum Thaker\\Desktop\\VSC\\jupyter\\myvenv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcf91d006fb7436da27378f41aae56db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sample: \n",
      "[2 2 2 2 2 0 2 1 0 1]\n",
      "Actual Sample: \n",
      "[2 2 1 2 2 0 1 1 0 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aum Thaker\\AppData\\Local\\Temp\\ipykernel_3088\\76004914.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  preds_prob = torch.nn.functional.softmax(torch.tensor(preds_float32), dim=-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5538648366928101, 'eval_accuracy': 0.8608695652173913, 'eval_precision_weighted': 0.8906753852121766, 'eval_precision_macro': 0.8270476817317537, 'eval_recall_weighted': 0.8608695652173913, 'eval_recall_macro': 0.9144793974154469, 'eval_f1_weighted': 0.8630055837539742, 'eval_f1_macro': 0.8571138160993629, 'eval_runtime': 3.7975, 'eval_samples_per_second': 90.85, 'eval_steps_per_second': 2.897, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aum Thaker\\Desktop\\VSC\\jupyter\\myvenv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\Aum Thaker\\Desktop\\VSC\\jupyter\\myvenv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b840d1bbe75148f8aa211496b147ce18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sample: \n",
      "[2 2 2 2 2 0 2 1 0 1]\n",
      "Actual Sample: \n",
      "[2 2 1 2 2 0 1 1 0 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aum Thaker\\AppData\\Local\\Temp\\ipykernel_3088\\76004914.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  preds_prob = torch.nn.functional.softmax(torch.tensor(preds_float32), dim=-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5019505023956299, 'eval_accuracy': 0.8782608695652174, 'eval_precision_weighted': 0.900883701728351, 'eval_precision_macro': 0.8399629559376499, 'eval_recall_weighted': 0.8782608695652174, 'eval_recall_macro': 0.9238251918079703, 'eval_f1_weighted': 0.8800283316988122, 'eval_f1_macro': 0.8710526315789474, 'eval_runtime': 3.9304, 'eval_samples_per_second': 87.777, 'eval_steps_per_second': 2.799, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aum Thaker\\Desktop\\VSC\\jupyter\\myvenv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\Aum Thaker\\Desktop\\VSC\\jupyter\\myvenv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a1fdd108fc04e3a9da66976d9a53e58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sample: \n",
      "[2 2 1 2 2 0 2 1 0 1]\n",
      "Actual Sample: \n",
      "[2 2 1 2 2 0 1 1 0 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aum Thaker\\AppData\\Local\\Temp\\ipykernel_3088\\76004914.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  preds_prob = torch.nn.functional.softmax(torch.tensor(preds_float32), dim=-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.45361778140068054, 'eval_accuracy': 0.8927536231884058, 'eval_precision_weighted': 0.9101394115506731, 'eval_precision_macro': 0.8543257289158929, 'eval_recall_weighted': 0.8927536231884058, 'eval_recall_macro': 0.9316133538017398, 'eval_f1_weighted': 0.8942400259908866, 'eval_f1_macro': 0.8845044307950242, 'eval_runtime': 3.7912, 'eval_samples_per_second': 90.999, 'eval_steps_per_second': 2.901, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aum Thaker\\Desktop\\VSC\\jupyter\\myvenv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\Aum Thaker\\Desktop\\VSC\\jupyter\\myvenv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34565c795e804cff88aa5fe3bfe7ff9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sample: \n",
      "[2 2 1 2 2 0 2 1 0 1]\n",
      "Actual Sample: \n",
      "[2 2 1 2 2 0 1 1 0 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aum Thaker\\AppData\\Local\\Temp\\ipykernel_3088\\76004914.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  preds_prob = torch.nn.functional.softmax(torch.tensor(preds_float32), dim=-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4154874384403229, 'eval_accuracy': 0.8956521739130435, 'eval_precision_weighted': 0.912066525879155, 'eval_precision_macro': 0.8567741094167101, 'eval_recall_weighted': 0.8956521739130435, 'eval_recall_macro': 0.9331709862004937, 'eval_f1_weighted': 0.8970830004778658, 'eval_f1_macro': 0.8868920143377537, 'eval_runtime': 3.8177, 'eval_samples_per_second': 90.368, 'eval_steps_per_second': 2.881, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aum Thaker\\Desktop\\VSC\\jupyter\\myvenv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\Aum Thaker\\Desktop\\VSC\\jupyter\\myvenv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b26633e03b3640a59690db5cd6105fc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sample: \n",
      "[2 2 1 2 2 0 2 1 0 1]\n",
      "Actual Sample: \n",
      "[2 2 1 2 2 0 1 1 0 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aum Thaker\\AppData\\Local\\Temp\\ipykernel_3088\\76004914.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  preds_prob = torch.nn.functional.softmax(torch.tensor(preds_float32), dim=-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3859139680862427, 'eval_accuracy': 0.9043478260869565, 'eval_precision_weighted': 0.9179747517803784, 'eval_precision_macro': 0.8698375916022975, 'eval_recall_weighted': 0.9043478260869565, 'eval_recall_macro': 0.9378438833967554, 'eval_f1_weighted': 0.9056027452056656, 'eval_f1_macro': 0.897419604817134, 'eval_runtime': 3.8081, 'eval_samples_per_second': 90.596, 'eval_steps_per_second': 2.889, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aum Thaker\\Desktop\\VSC\\jupyter\\myvenv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\Aum Thaker\\Desktop\\VSC\\jupyter\\myvenv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0adc44c9e0764d72be3acdd0be100726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sample: \n",
      "[2 2 1 2 2 0 2 1 0 1]\n",
      "Actual Sample: \n",
      "[2 2 1 2 2 0 1 1 0 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aum Thaker\\AppData\\Local\\Temp\\ipykernel_3088\\76004914.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  preds_prob = torch.nn.functional.softmax(torch.tensor(preds_float32), dim=-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.36250415444374084, 'eval_accuracy': 0.9043478260869565, 'eval_precision_weighted': 0.9179747517803784, 'eval_precision_macro': 0.8698375916022975, 'eval_recall_weighted': 0.9043478260869565, 'eval_recall_macro': 0.9378438833967554, 'eval_f1_weighted': 0.9056027452056656, 'eval_f1_macro': 0.897419604817134, 'eval_runtime': 3.7552, 'eval_samples_per_second': 91.872, 'eval_steps_per_second': 2.929, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aum Thaker\\Desktop\\VSC\\jupyter\\myvenv\\Lib\\site-packages\\peft\\utils\\other.py:611: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: ac80d556-3044-4b26-a41b-1f889edb721d)') - silently ignoring the lookup for the file config.json in soleimanian/financial-roberta-large-sentiment.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Aum Thaker\\Desktop\\VSC\\jupyter\\myvenv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:195: UserWarning: Could not find a config file in soleimanian/financial-roberta-large-sentiment - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Aum Thaker\\Desktop\\VSC\\jupyter\\myvenv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\Aum Thaker\\Desktop\\VSC\\jupyter\\myvenv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b1d9bcc97a14e03b7905c90fe0c24af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sample: \n",
      "[2 2 1 2 2 0 2 1 0 1]\n",
      "Actual Sample: \n",
      "[2 2 1 2 2 0 1 1 0 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aum Thaker\\AppData\\Local\\Temp\\ipykernel_3088\\76004914.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  preds_prob = torch.nn.functional.softmax(torch.tensor(preds_float32), dim=-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.34431618452072144, 'eval_accuracy': 0.9101449275362319, 'eval_precision_weighted': 0.9220009983028851, 'eval_precision_macro': 0.8749605670360387, 'eval_recall_weighted': 0.9101449275362319, 'eval_recall_macro': 0.9409591481942631, 'eval_f1_weighted': 0.9112713877175126, 'eval_f1_macro': 0.9022296544035675, 'eval_runtime': 3.7213, 'eval_samples_per_second': 92.71, 'eval_steps_per_second': 2.956, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aum Thaker\\Desktop\\VSC\\jupyter\\myvenv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\Aum Thaker\\Desktop\\VSC\\jupyter\\myvenv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8676ef6d4c7040159e3947fcbe78b607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sample: \n",
      "[2 2 1 2 2 0 2 1 0 1]\n",
      "Actual Sample: \n",
      "[2 2 1 2 2 0 1 1 0 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aum Thaker\\AppData\\Local\\Temp\\ipykernel_3088\\76004914.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  preds_prob = torch.nn.functional.softmax(torch.tensor(preds_float32), dim=-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3311152458190918, 'eval_accuracy': 0.9101449275362319, 'eval_precision_weighted': 0.9220009983028851, 'eval_precision_macro': 0.8749605670360387, 'eval_recall_weighted': 0.9101449275362319, 'eval_recall_macro': 0.9409591481942631, 'eval_f1_weighted': 0.9112713877175126, 'eval_f1_macro': 0.9022296544035675, 'eval_runtime': 3.7684, 'eval_samples_per_second': 91.55, 'eval_steps_per_second': 2.919, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aum Thaker\\Desktop\\VSC\\jupyter\\myvenv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\Aum Thaker\\Desktop\\VSC\\jupyter\\myvenv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b51ee853d5744620908aac78eebdf00e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sample: \n",
      "[2 2 1 2 2 0 2 1 0 1]\n",
      "Actual Sample: \n",
      "[2 2 1 2 2 0 1 1 0 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aum Thaker\\AppData\\Local\\Temp\\ipykernel_3088\\76004914.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  preds_prob = torch.nn.functional.softmax(torch.tensor(preds_float32), dim=-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3225196301937103, 'eval_accuracy': 0.9101449275362319, 'eval_precision_weighted': 0.9220009983028851, 'eval_precision_macro': 0.8749605670360387, 'eval_recall_weighted': 0.9101449275362319, 'eval_recall_macro': 0.9409591481942631, 'eval_f1_weighted': 0.9112713877175126, 'eval_f1_macro': 0.9022296544035675, 'eval_runtime': 3.8389, 'eval_samples_per_second': 89.869, 'eval_steps_per_second': 2.865, 'epoch': 13.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aum Thaker\\Desktop\\VSC\\jupyter\\myvenv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\Aum Thaker\\Desktop\\VSC\\jupyter\\myvenv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1347a5821b254af8867208fd39af2d6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sample: \n",
      "[2 2 1 2 2 0 2 1 0 1]\n",
      "Actual Sample: \n",
      "[2 2 1 2 2 0 1 1 0 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aum Thaker\\AppData\\Local\\Temp\\ipykernel_3088\\76004914.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  preds_prob = torch.nn.functional.softmax(torch.tensor(preds_float32), dim=-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3176673352718353, 'eval_accuracy': 0.9101449275362319, 'eval_precision_weighted': 0.9204796939671032, 'eval_precision_macro': 0.8777152139791674, 'eval_recall_weighted': 0.9101449275362319, 'eval_recall_macro': 0.934580272656509, 'eval_f1_weighted': 0.9112129595221384, 'eval_f1_macro': 0.9015511237733459, 'eval_runtime': 3.7846, 'eval_samples_per_second': 91.159, 'eval_steps_per_second': 2.907, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aum Thaker\\Desktop\\VSC\\jupyter\\myvenv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\Aum Thaker\\Desktop\\VSC\\jupyter\\myvenv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc454defde4d4fc1836fac601596b194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sample: \n",
      "[2 2 1 2 2 0 2 1 0 1]\n",
      "Actual Sample: \n",
      "[2 2 1 2 2 0 1 1 0 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aum Thaker\\AppData\\Local\\Temp\\ipykernel_3088\\76004914.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  preds_prob = torch.nn.functional.softmax(torch.tensor(preds_float32), dim=-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.31573766469955444, 'eval_accuracy': 0.9101449275362319, 'eval_precision_weighted': 0.9204796939671032, 'eval_precision_macro': 0.8777152139791674, 'eval_recall_weighted': 0.9101449275362319, 'eval_recall_macro': 0.934580272656509, 'eval_f1_weighted': 0.9112129595221384, 'eval_f1_macro': 0.9015511237733459, 'eval_runtime': 3.9233, 'eval_samples_per_second': 87.937, 'eval_steps_per_second': 2.804, 'epoch': 15.0}\n",
      "{'train_runtime': 8292.1953, 'train_samples_per_second': 4.996, 'train_steps_per_second': 0.02, 'train_loss': 0.4390633322975852, 'epoch': 15.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=165, training_loss=0.4390633322975852, metrics={'train_runtime': 8292.1953, 'train_samples_per_second': 4.996, 'train_steps_per_second': 0.02, 'total_flos': 1.022036761427724e+16, 'train_loss': 0.4390633322975852, 'epoch': 15.0})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error resolved after 3:03:57.969446, resuming normal operation.\n",
      "wandb: Network error resolved after 0:01:27.345137, resuming normal operation.\n",
      "wandb: Network error resolved after 0:01:34.681490, resuming normal operation.\n",
      "wandb: Network error resolved after 1:42:10.976160, resuming normal operation.\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e57cdfdd54464c2ca99ea035a50d5292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sample: \n",
      "[0 1 0 0 0 1 1 2 2 2]\n",
      "Actual Sample: \n",
      "[0 1 0 0 1 1 1 2 2 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aum Thaker\\AppData\\Local\\Temp\\ipykernel_3088\\76004914.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  preds_prob = torch.nn.functional.softmax(torch.tensor(preds_float32), dim=-1)\n"
     ]
    }
   ],
   "source": [
    "test_results = trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c37c66a0cc44c3c97e8fbdb6207d70f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sample: \n",
      "[0 1 0 0 0 1 1 2 2 2]\n",
      "Actual Sample: \n",
      "[0 1 0 0 1 1 1 2 2 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aum Thaker\\AppData\\Local\\Temp\\ipykernel_3088\\76004914.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  preds_prob = torch.nn.functional.softmax(torch.tensor(preds_float32), dim=-1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.2725694477558136,\n",
       " 'eval_accuracy': 0.9219653179190751,\n",
       " 'eval_precision_weighted': 0.9332768443366245,\n",
       " 'eval_precision_macro': 0.8964210324313951,\n",
       " 'eval_recall_weighted': 0.9219653179190751,\n",
       " 'eval_recall_macro': 0.940977432694182,\n",
       " 'eval_f1_weighted': 0.923649618171909,\n",
       " 'eval_f1_macro': 0.9146551438125865,\n",
       " 'eval_runtime': 3.9592,\n",
       " 'eval_samples_per_second': 87.391,\n",
       " 'eval_steps_per_second': 2.778,\n",
       " 'epoch': 15.0}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredictionOutput(predictions=array([[ 4.4882812 , -2.0859375 , -1.7275391 ],\n",
      "       [-3.4453125 ,  4.9648438 , -2.4882812 ],\n",
      "       [ 4.8085938 , -1.9384766 , -2.3222656 ],\n",
      "       ...,\n",
      "       [-2.7558594 ,  4.9453125 , -3.15625   ],\n",
      "       [-2.421875  , -0.83740234,  4.0351562 ],\n",
      "       [-3.5625    ,  5.0429688 , -2.390625  ]], dtype=float32), label_ids=array([0, 1, 0, 0, 1, 1, 1, 2, 2, 2, 2, 1, 1, 2, 0, 0, 1, 1, 2, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 0, 2, 2, 2, 1, 1, 2, 1, 1, 2, 1, 2, 1, 2, 0, 0, 1,\n",
      "       0, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       2, 2, 1, 1, 1, 2, 2, 1, 1, 2, 1, 2, 2, 1, 1, 1, 1, 0, 1, 1, 0, 2,\n",
      "       1, 1, 1, 1, 2, 1, 0, 1, 0, 1, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 0, 1,\n",
      "       1, 1, 1, 2, 2, 2, 2, 1, 0, 1, 2, 1, 2, 0, 1, 2, 2, 1, 2, 2, 1, 1,\n",
      "       2, 1, 2, 1, 1, 1, 0, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1,\n",
      "       1, 1, 2, 2, 2, 2, 0, 1, 1, 2, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
      "       2, 2, 2, 2, 0, 0, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0,\n",
      "       2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 0,\n",
      "       1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 2, 1, 2, 1, 2, 1, 1, 0, 1, 1,\n",
      "       0, 2, 0, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2,\n",
      "       1, 0, 1, 1, 2, 0, 1, 1, 1, 0, 0, 1, 1, 2, 1, 2, 0, 1, 1, 0, 1, 1,\n",
      "       1, 1, 2, 2, 1, 1, 1, 2, 2, 1, 1, 0, 2, 2, 0, 1, 2, 2, 1, 1, 1, 2,\n",
      "       1, 1, 1, 2, 1, 1, 1, 1, 0, 0, 1, 1, 2, 1, 1, 2, 1, 1, 1, 0, 1, 2,\n",
      "       1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 0, 2, 1, 1, 2, 1], dtype=int64), metrics={'test_loss': 0.2725694477558136, 'test_accuracy': 0.9219653179190751, 'test_precision_weighted': 0.9332768443366245, 'test_precision_macro': 0.8964210324313951, 'test_recall_weighted': 0.9219653179190751, 'test_recall_macro': 0.940977432694182, 'test_f1_weighted': 0.923649618171909, 'test_f1_macro': 0.9146551438125865, 'test_runtime': 3.9718, 'test_samples_per_second': 87.114, 'test_steps_per_second': 2.77})\n"
     ]
    }
   ],
   "source": [
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(r'Sentiment_Roberta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Sentiment_Roberta\\\\tokenizer_config.json',\n",
       " 'Sentiment_Roberta\\\\special_tokens_map.json',\n",
       " 'Sentiment_Roberta\\\\vocab.json',\n",
       " 'Sentiment_Roberta\\\\merges.txt',\n",
       " 'Sentiment_Roberta\\\\added_tokens.json',\n",
       " 'Sentiment_Roberta\\\\tokenizer.json')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(r'Sentiment_Roberta')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "k_",
   "language": "python",
   "name": "k_"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
